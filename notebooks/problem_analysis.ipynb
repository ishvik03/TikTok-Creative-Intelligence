{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b4c6e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d92d08c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "data_path = \"../data/processed/cleaned_tiktok_data.csv\"\n",
    "output_path = \"../insights/problems.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e48d190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned TikTok data\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7b62534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Use the correct text source\n",
    "text_col = \"text\" if \"text\" in df.columns else df.columns[df.columns.str.contains(\"desc|caption|transcript\", case=False)].tolist()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4d3953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa666aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common problems (expand this list over time)\n",
    "problem_keywords = [\n",
    "    \"acne\", \"dry skin\", \"oily skin\", \"dark circles\", \"hair loss\",\n",
    "    \"burnout\", \"stress\", \"overthinking\", \"low energy\", \"depression\",\n",
    "    \"bad wifi\", \"slow internet\", \"sensitive skin\", \"anxiety\", \"breakouts\",\n",
    "    \"insomnia\", \"thin hair\", \"bloating\", \"brain fog\", \"irritation\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fd484a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase everything for matching\n",
    "problem_keywords = [kw.lower() for kw in problem_keywords]\n",
    "\n",
    "# Function to find problems mentioned\n",
    "def extract_problems(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    return [kw for kw in problem_keywords if kw in text]\n",
    "\n",
    "# Apply tagging\n",
    "df[\"mentioned_problems\"] = df[text_col].apply(extract_problems)\n",
    "df[\"has_problem\"] = df[\"mentioned_problems\"].apply(lambda x: len(x) > 0)\n",
    "\n",
    "# Filter down to relevant columns\n",
    "problem_df = df[[\"text\", \"mentioned_problems\", \"has_problem\", \"country\", \"is_viral\"]].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05820eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Problems extracted and saved to: ../insights/problems.csv\n"
     ]
    }
   ],
   "source": [
    "# Save output\n",
    "os.makedirs(\"../insights\", exist_ok=True)\n",
    "problem_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Problems extracted and saved to: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "184cf120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>mentioned_problems</th>\n",
       "      <th>has_problem</th>\n",
       "      <th>country</th>\n",
       "      <th>is_viral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>social anxiety fears me #outfit #milano #italy...</td>\n",
       "      <td>[anxiety]</td>\n",
       "      <td>True</td>\n",
       "      <td>Italy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text mentioned_problems  \\\n",
       "213  social anxiety fears me #outfit #milano #italy...          [anxiety]   \n",
       "\n",
       "     has_problem country  is_viral  \n",
       "213         True   Italy     False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_df[problem_df[\"has_problem\"]].sample(min(5, problem_df[\"has_problem\"].sum()), random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b86d2c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('anxiety', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "problem_counts = Counter(chain.from_iterable(problem_df[\"mentioned_problems\"]))\n",
    "print(problem_counts.most_common())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a80bfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keybert\n",
      "  Downloading keybert-0.9.0-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m530.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keybert) (1.26.4)\n",
      "Collecting sentence-transformers>=0.3.8\n",
      "  Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m470.2/470.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keybert) (1.5.2)\n",
      "Requirement already satisfied: rich>=10.4.0 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keybert) (13.7.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rich>=10.4.0->keybert) (2.18.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from scikit-learn>=0.22.2->keybert) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from scikit-learn>=0.22.2->keybert) (1.13.1)\n",
      "Requirement already satisfied: tqdm in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from sentence-transformers>=0.3.8->keybert) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from sentence-transformers>=0.3.8->keybert) (2.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from sentence-transformers>=0.3.8->keybert) (4.12.2)\n",
      "Requirement already satisfied: Pillow in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from sentence-transformers>=0.3.8->keybert) (10.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from sentence-transformers>=0.3.8->keybert) (0.24.6)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from sentence-transformers>=0.3.8->keybert) (4.44.2)\n",
      "Requirement already satisfied: requests in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
      "Requirement already satisfied: filelock in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (6.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (24.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
      "Requirement already satisfied: jinja2 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.4)\n",
      "Requirement already satisfied: networkx in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.3)\n",
      "Requirement already satisfied: sympy in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.19.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2024.7.24)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.7.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.7)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/zubair/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
      "Installing collected packages: sentence-transformers, keybert\n",
      "Successfully installed keybert-0.9.0 sentence-transformers-5.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keybert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77bd13ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 01:45:43.030513: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3dc3c29122741b192dd690c70fffee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fda0fa18d64ee6b669347c1ef0a98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1650dd6295ff4edf814622ba861e1261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9470c705a9940efb2d187b3a881cd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4562a4bef8424653bf503652d8b1c925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8605b5c3874f9eb0f0dbcc5d260944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d8c2321c9541399ac2b2b592e2ec20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28aa7a0e2dbc41eaa913cbba81b052a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bf4949a6b54c19b1290ed31f6f8c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249b9f62f36f456c917c504df0d7f046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4152d7e0f744f558d1e297c6941b57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>keyphrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>みんなのおすすめなTシャツブランド教えて\\n#street #ootd #outfit #f...</td>\n",
       "      <td>[(みんなのおすすめなtシャツブランド教えて, 0.7021), (ストリートファッション,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>夏服に迷ってる人必見個人的におすすめのTシャツ6選✨\\n\\n#ファッション #ストリートファ...</td>\n",
       "      <td>[(夏服に迷ってる人必見個人的におすすめのtシャツ6選, 0.8537), (夏服, 0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>とうとう無地Tの季節がやってきましたね！\\nみなさん好きな無地Tブランドはなんですか！\\n#...</td>\n",
       "      <td>[(みなさん好きな無地tブランドはなんですか, 0.8022), (とうとう無地tの季節がや...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>綺麗なAラインシルエットが作れるコスパ最強デニムはここ#デニム #バギーデニム #ストリート...</td>\n",
       "      <td>[(綺麗なaラインシルエットが作れるコスパ最強デニムはここ, 0.8916), (バギーデニ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ストリートファッション女性編\\n#ファッション #おしゃれ #cityboy #ストリートフ...</td>\n",
       "      <td>[(ストリートファッション女性編, 0.7955), (ストリートファッション, 0.723...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  みんなのおすすめなTシャツブランド教えて\\n#street #ootd #outfit #f...   \n",
       "1  夏服に迷ってる人必見個人的におすすめのTシャツ6選✨\\n\\n#ファッション #ストリートファ...   \n",
       "2  とうとう無地Tの季節がやってきましたね！\\nみなさん好きな無地Tブランドはなんですか！\\n#...   \n",
       "3  綺麗なAラインシルエットが作れるコスパ最強デニムはここ#デニム #バギーデニム #ストリート...   \n",
       "4  ストリートファッション女性編\\n#ファッション #おしゃれ #cityboy #ストリートフ...   \n",
       "\n",
       "                                          keyphrases  \n",
       "0  [(みんなのおすすめなtシャツブランド教えて, 0.7021), (ストリートファッション,...  \n",
       "1  [(夏服に迷ってる人必見個人的におすすめのtシャツ6選, 0.8537), (夏服, 0.6...  \n",
       "2  [(みなさん好きな無地tブランドはなんですか, 0.8022), (とうとう無地tの季節がや...  \n",
       "3  [(綺麗なaラインシルエットが作れるコスパ最強デニムはここ, 0.8916), (バギーデニ...  \n",
       "4  [(ストリートファッション女性編, 0.7955), (ストリートファッション, 0.723...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"../data/processed/cleaned_tiktok_data.csv\")\n",
    "df = df[df[\"text\"].notna()]  # Drop empty rows\n",
    "\n",
    "# Init model\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "# Run keyword extraction on a sample of posts\n",
    "df[\"keyphrases\"] = df[\"text\"].apply(lambda x: kw_model.extract_keywords(x, top_n=3, stop_words='english'))\n",
    "\n",
    "# Preview\n",
    "df[[\"text\", \"keyphrases\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42b37e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_words = [\n",
    "    \"迷ってる\", \"困る\", \"嫌い\", \"悩み\", \"ストレス\", \"失敗\", \"できない\", \"難しい\",  # Japanese\n",
    "    \"confused\", \"stressed\", \"hate\", \"problem\", \"struggle\", \"fail\", \"cannot\", \"hard\", \"bad\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46db9ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_problem_phrase(phrases):\n",
    "    return any(\n",
    "        any(word in kw for word in problem_words)\n",
    "        for kw, _ in phrases if isinstance(kw, str)\n",
    "    )\n",
    "\n",
    "df[\"is_problem\"] = df[\"keyphrases\"].apply(is_problem_phrase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "031db82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved problem analysis (NLP) to problems_nlp.csv\n"
     ]
    }
   ],
   "source": [
    "df[[\"text\", \"keyphrases\", \"is_problem\"]].to_csv(\"../insights/problems_nlp.csv\", index=False)\n",
    "print(\"✅ Saved problem analysis (NLP) to problems_nlp.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f3dbcb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_problem\n",
       "False    380\n",
       "True       2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"is_problem\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd3345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
